The requirements described in \cref{ReqCha} \cref{Req} were achieved completely. We installed Pelias and all of its’ services as a docker image as well as a from scratch installation. The Team built a complete map of Europe based on data provided by OpenStreetmaps, WhosOnFirst and OpenAddresses. Furthermore, we calculated tiles and polylines based on OpenStreetmaps data. This process initially failed, since calculating polylines for complete Europe requires more than 32 GB of RAM-Memory. After an upgrade of the virtual machine to 64 GB this step finished successfully. During the calculation and import steps several bugs in Pelias were found and submitted. Luckily, they were fixed, or workarounds were provided within a few days.\\
The postcode data was gathered from Geonames.org and Postcode.info. The data from Geonames.org can be downloaded as ZIP-files separated in Countries, or as one single ZIP-file which contains the whole world. The data from Postcode.info had to be scraped from their website by our fellow student Ankur Mehra. After the data from Geonames.org and Postcode.info had been merged, we could calculate the 2-digit postcodes and import our newly generated data basis as a custom data-source into Pelias and Elasticsearch. On this data basis we can now do geocoding in Pelias for 2-digit postcodes in Europe.
We did research on several routing engines and decided to use Valhalla as an alternative to the already existing Graphhopper which is used by the senior team. Valhalla offers similar performance for cached data/routes (including start and end points in the vicinity of previously used coordinates) as Graphhopper with a good trade-off between resource requirements and performance/time to calculate a route. Furthermore, polylines calculated from OpenStreetmaps data with Pelias' tools can be used in Valhalla and vice versa.\\
All in all, we can say, that the first semester of our project has been successful, and we reached our overall goal of testing Pelias as an alternative to Nominatim. Most of our tasks were finished, although some tasks couldn’t be finished during the last sprint. Those tasks are mainly concerned about a VPN connection to the cluster of computers located at campus Albert-Einstein-Allee. As soon as those tasks are finished successfully, we could proceed with installing an instance of our Pelias build on the cluster and run this build as some kind of production-system. However in the coming project phase we will probably be more focused on data analytics and machine learning depending on the product owners requirements.